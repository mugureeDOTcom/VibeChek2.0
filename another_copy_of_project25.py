# -*- coding: utf-8 -*-
"""Another copy of Project25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DL1v3OMmVasYFdlJLFeVzsR6nMJiWkgO
"""



!pip install google-search-results pandas nltk transformers streamlit

from serpapi import GoogleSearch
import pandas as pd
import time

# 🔑 Replace with your actual SerpAPI Key
SERPAPI_KEY = "af0c1424e01dc7212486864831658bf9c965bd36fa5b3cc5ee802f16a2d94163"

# 🎯 Replace with the Place ID you got
PLACE_ID = "ChIJTydumocALxgRV5VfnxxO4Cs"

# 🔄 Fetch Up to 3,000 Reviews
all_reviews = []
start = 0
max_reviews = 500  # 🔥 Set limit to exactly 3,000

while len(all_reviews) < max_reviews:
    print(f"🔍 Fetching reviews starting from index {start}...")

    params = {
        "engine": "google_maps_reviews",
        "place_id": PLACE_ID,
        "api_key": SERPAPI_KEY,
        "start": start
    }

    search = GoogleSearch(params)
    results = search.get_dict()

    # Extract reviews
    reviews = results.get("reviews", [])
    if not reviews:
        print("✅ No more reviews left. Stopping.")
        break  # Stop when no more reviews are available

    all_reviews.extend(reviews)  # Add reviews to our list

    start += len(reviews)  # Move to the next batch
    time.sleep(2)  # Prevent hitting API rate limits

    # 🔥 Save in Batches (Every 500 reviews)
    if len(all_reviews) % 500 == 0 or len(all_reviews) >= max_reviews:
        df = pd.DataFrame(all_reviews[:max_reviews])  # Only keep max 3,000
        df.to_csv("reviews.csv", index=False)
        print(f"✅ Saved {len(df)} reviews so far...")

# 🚀 Final Save
df = pd.DataFrame(all_reviews[:max_reviews])  # Ensure it's exactly 3,000
df.to_csv("google_reviews.csv", index=False)

# 🎯 Show Total Reviews
print(f"✅ Total reviews scraped: {len(df)}")
df.head()

import pandas as pd
import re

# 📌 Load the scraped reviews
df = pd.read_csv("reviews.csv")

# 🔍 Check available columns
print("✅ Available Columns:", df.columns)

# ✅ Set the correct column for reviews
actual_col_name = "snippet"

# 🔄 Cleaning function
def clean_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    text = text.replace("\n", " ")
    return text.strip().lower()

df["Cleaned_Review"] = df[actual_col_name].astype(str).apply(clean_text)

# 🚀 Save Cleaned Data
df.to_csv("cleaned_reviews.csv", index=False)

# 🎯 Show Sample Cleaned Reviews
print("✅ Final Cleaning Complete! First 5 Cleaned Reviews:")
df[["snippet", "Cleaned_Review"]].head()

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline

# 📌 Load the cleaned reviews
df = pd.read_csv("cleaned_reviews.csv")

# ✅ Initialize Sentiment Analyzers
nltk.download("vader_lexicon")
sia = SentimentIntensityAnalyzer()
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# 🎯 Function to categorize sentiment based on star rating
def categorize_star_rating(rating):
    if rating >= 4.0:
        return "Positive"
    elif rating == 3.0:
        return "Neutral"
    else:
        return "Negative"

# 🎯 Function to classify sentiment using VADER
def analyze_sentiment(text):
    score = sia.polarity_scores(text)["compound"]
    if score >= 0.05:
        return "Positive"
    elif score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

# 🎯 Function to classify sentiment using BERT (Swahili + English)
def analyze_multilingual_sentiment(text):
    result = sentiment_pipeline(text)[0]["label"]
    return "Positive" if "5" in result or "4" in result else "Negative" if "1" in result or "2" in result else "Neutral"

# 🔥 Apply Sentiment Analysis
df["Star_Category"] = df["rating"].apply(categorize_star_rating)
df["Sentiment_VADER"] = df["Cleaned_Review"].apply(analyze_sentiment)
df["Sentiment_BERT"] = df["Cleaned_Review"].apply(analyze_multilingual_sentiment)

# 🚀 Save Sentiment Results
df.to_csv("sentiment_results.csv", index=False)

# 🎯 Show Sample Sentiment Analysis Results
print("✅ Sentiment Analysis Complete! First 5 Reviews:")
df[["rating", "Star_Category", "Cleaned_Review", "Sentiment_VADER", "Sentiment_BERT"]].head()

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# 📌 Load sentiment results
df = pd.read_csv("sentiment_results.csv")

# ✅ Generate a Word Cloud for Positive Reviews
positive_words = " ".join(df[df["Sentiment_VADER"] == "Positive"]["Cleaned_Review"])
wordcloud_pos = WordCloud(width=800, height=400, background_color="white").generate(positive_words)

# ✅ Generate a Word Cloud for Negative Reviews
negative_words = " ".join(df[df["Sentiment_VADER"] == "Negative"]["Cleaned_Review"])
wordcloud_neg = WordCloud(width=800, height=400, background_color="black").generate(negative_words)

# 🎯 Show Word Clouds
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_pos, interpolation="bilinear")
plt.title("Most Used Words in Positive Reviews")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(wordcloud_neg, interpolation="bilinear")
plt.title("Most Used Words in Negative Reviews")
plt.axis("off")

plt.show()

from collections import Counter
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already downloaded
nltk.download('stopwords')

# Get English stopwords
stop_words = set(stopwords.words('english'))
# You can add more domain-specific stopwords if needed
stop_words.update(['also', 'would', 'could', 'get', 'even'])

# ✅ Extract all words from Positive & Negative reviews (excluding stopwords)
positive_text = [word for word in " ".join(df[df["Sentiment_VADER"] == "Positive"]["Cleaned_Review"]).split()
                if word.lower() not in stop_words]
negative_text = [word for word in " ".join(df[df["Sentiment_VADER"] == "Negative"]["Cleaned_Review"]).split()
                if word.lower() not in stop_words]

# ✅ Count word frequencies
positive_words_count = Counter(positive_text).most_common(10)
negative_words_count = Counter(negative_text).most_common(10)

# 🎯 Show the top words
print("✅ Top 10 Meaningful Words in Positive Reviews:")
print(positive_words_count)

print("\n✅ Top 10 Meaningful Words in Negative Reviews:")
print(negative_words_count)

import matplotlib.pyplot as plt
import seaborn as sns

# 📌 Load sentiment results
df = pd.read_csv("sentiment_results.csv")

# ✅ Convert `iso_date` column to Date format
df["iso_date"] = pd.to_datetime(df["iso_date"])

# Add month column
df["month"] = df["iso_date"].dt.to_period("M")

# ✅ Group sentiment counts by month
monthly_sentiment = df.groupby(["month", "Sentiment_VADER"]).size().unstack()

# ✅ Fill missing values with 0
monthly_sentiment = monthly_sentiment.fillna(0)

# 🎯 Plot Monthly Sentiment Trends
plt.figure(figsize=(14, 6))
monthly_sentiment.plot(kind="bar",
                      color={"Positive": "green", "Neutral": "gray", "Negative": "red"},
                      figsize=(14, 6))

plt.title("📊 Monthly Sentiment Distribution")
plt.xlabel("Month")
plt.ylabel("Number of Reviews")
plt.legend(title="Sentiment")
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# 📌 Load the sentiment results
df = pd.read_csv("sentiment_results.csv")

# ✅ Extract Negative & Positive reviews
negative_reviews = df[df["Sentiment_VADER"] == "Negative"]["Cleaned_Review"]
positive_reviews = df[df["Sentiment_VADER"] == "Positive"]["Cleaned_Review"]

# ✅ Convert Negative Reviews into a numerical format
vectorizer_neg = CountVectorizer(stop_words="english", max_features=1000)
X_neg = vectorizer_neg.fit_transform(negative_reviews)

# ✅ Apply LDA Topic Modeling to Negative Reviews
lda_neg = LatentDirichletAllocation(n_components=3, random_state=42)
lda_neg.fit(X_neg)

# 🎯 Extract Key Issues from Negative Reviews
negative_issues = []
print("\n✅ Key Issues in Negative Reviews:")
for i, topic in enumerate(lda_neg.components_):
    words = [vectorizer_neg.get_feature_names_out()[index] for index in topic.argsort()[-10:]]
    negative_issues.append(", ".join(words))
    print(f"🔴 Topic {i+1}: {', '.join(words)}")

# ✅ Convert Positive Reviews into a numerical format
vectorizer_pos = CountVectorizer(stop_words="english", max_features=1000)
X_pos = vectorizer_pos.fit_transform(positive_reviews)

# ✅ Apply LDA Topic Modeling to Positive Reviews
lda_pos = LatentDirichletAllocation(n_components=3, random_state=42)
lda_pos.fit(X_pos)

# 🎯 Extract Key Strengths from Positive Reviews
positive_strengths = []
print("\n✅ Key Strengths in Positive Reviews:")
for i, topic in enumerate(lda_pos.components_):
    words = [vectorizer_pos.get_feature_names_out()[index] for index in topic.argsort()[-10:]]
    positive_strengths.append(", ".join(words))
    print(f"🟢 Topic {i+1}: {', '.join(words)}")

from transformers import pipeline

# ✅ Load AI Text Generator (GPT-based model)
generator = pipeline("text-generation", model="EleutherAI/gpt-neo-1.3B")

# 🔴 Generate Recommendations for Key Negative Review Issues
negative_recommendations = []
for issue in negative_issues:
    prompt = f"Customers have complained about {issue}. What are the best ways a business can improve this?"
    recommendation = generator(prompt, max_length=1000, do_sample=True)[0]["generated_text"]
    negative_recommendations.append(recommendation)

# 🟢 Generate Recommendations for Key Positive Review Strengths
positive_recommendations = []
for strength in positive_strengths:
    prompt = f"Customers love {strength}. How can a business continue excelling in this area?"
    recommendation = generator(prompt, max_length=1000, do_sample=True)[0]["generated_text"]
    positive_recommendations.append(recommendation)

# 🚀 Save Recommendations to a File
with open("recommendations.txt", "w") as file:
    file.write("📌 **Business Recommendations Based on Customer Feedback**\n\n")

    file.write("🔴 **Areas for Improvement:**\n")
    for rec in negative_recommendations:
        file.write(f"- {rec}\n\n")

    file.write("🟢 **Strengths to Maintain:**\n")
    for rec in positive_recommendations:
        file.write(f"- {rec}\n\n")

print("✅ AI-Based Recommendations Generated Successfully!")
print(f"Full recommendations saved to : reccommendaitons.txt")


print("\n📄 PREVIEW OF SAVED FILE:")
with open("recommendations.txt", "r") as file:
    print(file.read())